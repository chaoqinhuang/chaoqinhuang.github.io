<!DYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chaoqin Huang</title>
  
  <meta name="author" content="Chaoqin Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name> Chaoqin Huang (黄潮钦)</name>
              </p>
              <p>I am currently a joint PhD candidate at <a href="https://mediabrain.sjtu.edu.cn/">Shanghai Jiao Tong University</a> and <a href="http://www.lv-nus.org/"> National University of Singapore</a>, under the supervision of <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Prof. Ya Zhang</a> and <a href="https://sites.google.com/site/sitexinchaowang/">Assistant Prof. Xinchao Wang</a>. I received my bachelor's degree from Shanghai Jiao Tong University in June 2019, during which I worked as a research assistant at <a href="https://www.mvig.org/">MVIG</a> under the guidance of <a href="https://scholar.google.com.hk/citations?user=QZVQEWAAAAAJ&hl=zh-CN">Prof. Cewu Lu</a>.
              </p>
	      <p>
		      I am currently focused on anomaly detection for industrial defect identification and medical diagnosis.
              </p>      
              <a href="chaoqinhuang.pdf">CV</a> &nbsp/&nbsp
              <a href="mailto:huangchaoqin@sjtu.edu.cn">Email</a> &nbsp/&nbsp
              <a href="https://scholar.google.com/citations?user=BAZSE7wAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
              <a href="https://github.com/chaoqinhuang">Github</a> &nbsp/&nbsp
              <a href="./index_zh.html">Chinese Homepage (中文主页)</a>
              <p style="text-align:center">
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="image/chaoqinhuang01.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="image/chaoqinhuang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
	

  
</tbody></table>
	<br />
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
            <tr>
              <td>
                <h2>Awards</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="0"><tbody>
            <li> [2023/06]: VAND Runner-up Winner in CVPR 2023<br> 
            <li> [2022/12]: <strong>National Scholarship of China</strong><br> 
            <li> [2019/06]: Outstanding Graduate of Shanghai Jiao Tong University <br> 
            <li> [2019/06]: 1st Outstanding Graduation Thesis Award of Computer Science in SJTU <br>
            <li> [2018/12]: SenseTime Scholarship <br> 
            <li> [2018/05]: Shanghai Jiao Tong University WISH Scholarship <br> 
            <li> [2018/04]: Meritorious Winner of Mathematical Contest In Modeling 2018 <br>
            <li> [2017-2022]: Shanghai Jiao Tong University Academic Excellent Scholarship <br> <br> 
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
            <tr>
              <td>
                <h2>Services</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="0"><tbody>
          <li> Conference Reviewer: CVPR, ICCV, ECCV, NeruIPS, ICML, ICLR, MICCAI <br>
          <li> Journal Reviewer: IEEE transactions on image processing (TIP), IEEE Transactions on Multimedia (TMM), IEEE transactions on neural networks and learning systems (TNNLS), IEEE Transactions on Medical Imaging (TMI), Expert systems with applications, Information fusion <br> <br> 
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading><b></bold>Challenges</b></heading>
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/vand.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2308.04789">
                <papertitle>Multi-Scale Memory Comparison for Zero-/Few-Shot Anomaly Detection</papertitle>
              </a>
              <br>
              <strong>Chaoqin Huang*</strong>, 
              <a href="https://jjjaaafff.github.io/">Aofan Jiang*</a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a> (* Equal Contribution)
              <br>
                <em><a href="https://sites.google.com/view/vand-cvpr23/challenge">VAND</a> Runner-up Winner in CVPR</em> 2023
              <p>
              The approach employs a global memory bank to capture features across the entire image, while an individual memory bank focuses on simplified scenes containing a single object. The efficacy of our method is validated by its remarkable achievement of 4th place in the zero-shot track and 2nd place in the few-shot track of the Visual Anomaly and Novelty Detection (VAND) competition.
              </p>
            </td>

            </tr>
          </tr> 
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading><b></bold>Conferences</b></heading>
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/MVFA_CVPR.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2403.12570">
                <papertitle>Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images</papertitle>
              </a>
              <br>
              <strong>Chaoqin Huang*</strong>, 
              <a href="https://jjjaaafff.github.io/">Aofan Jiang*</a>, Jinghao Feng,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://sites.google.com/site/sitexinchaowang/">Xinchao Wang</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a> (* Equal Contribution)
              <br>
                <em>IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR)</em> 2024 <a href="https://github.com/MediaBrain-SJTU/MVFA-AD">code</a>
              <p>
              This paper introduces a lightweight multi-level adaptation and comparison framework to repurpose the CLIP model for medical anomaly detection. Our approach integrates multiple residual adapters into the pre-trained visual encoder, enabling a stepwise enhancement of visual features across different levels. The adapted features exhibit improved generalization across various medical data types, even in zero-shot scenarios where the model encounters unseen medical modalities and anatomical regions during training.
            </td>



          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/ECGAD.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2308.01639">
                <papertitle>Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection</papertitle>
              </a>
              <br>
              <a href="https://jjjaaafff.github.io/">Aofan Jiang*</a>,
              <strong>Chaoqin Huang*</strong>, 
              Qing Cao, Shuang Wu, Zi Zeng, Kang Chen,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a> (* Equal Contribution)
              <br>
                <em>International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</em> 2023 (Early accepted) <a href="https://github.com/MediaBrain-SJTU/ECGAD">code</a>
              <p>
              Detecting anomalies in Electrocardiogram (ECG) data is particularly challenging due to the substantial inter-individual differences and the presence of anomalies in both global rhythm and local morphology. Imitating the diagnostic process followed by experienced cardiologists, we proposed a novel multi-scale cross-restoration framework for ECG anomaly detection and localization, achieving state-of-the-art performance on our proposed large-scale ECG benchmark and two other well-known ECG datasets.
              </p>
            </td>
         
            <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()"  bgcolor="#ffffd0">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/RegAD.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-031-20053-3_18">
                <papertitle>Registration based Few-Shot Anomaly Detection</papertitle>
              </a>
              <br>
              <strong>Chaoqin Huang</strong>,
              <a href="https://scholar.google.com/citations?user=Z4bUA3EAAAAJ&hl=en">Haoyan Guan</a>,
              <a href="https://jjjaaafff.github.io/">Aofan Jiang</a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://nms.kcl.ac.uk/michael.spratling/">Michael Spratling</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>
              <br>
                <em>European Conference on Computer Vision (ECCV) </em> 2022 <font color="red"><strong>(Oral Presentation)</strong></font> <a href="https://github.com/MediaBrain-SJTU/RegAD">code</a>
              <p>
              We considered few-shot anomaly detection (FSAD), a practical yet under-studied setting for anomaly detection, where only a limited number of normal images are provided for each category at training. Inspired by how humans detect anomalies, i.e., comparing an image in question to normal images, we leveraged registration, an image alignment task that is inherently generalizable across categories, as the proxy task, to train a category-agnostic anomaly detection model. During testing, the anomalies are identified by comparing the registered features of the test image and its corresponding support (normal) images. Experimental results have shown that the proposed method outperforms the state-of-the-art FSAD methods by 3%-8% in AUC on the MVTec and MPDD benchmarks.
              </p>
            </td>

            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/DG_ISBI.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9761561">
                <papertitle>Semi-Supervised Domain Generalization for Medical Image Analysis</papertitle>
              </a>
              <br>
              <a href="https://frankzhangrp.github.io/">Ruipeng Zhang</a>,
              <a href="https://scholar.google.com/citations?user=hQJ5hwMAAAAJ&hl=en&oi=ao">Qinwei Xu</a>,
              <strong>Chaoqin Huang</strong>, 
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>
              <br>
                <em>IEEE International Symposium on Biomedical Imaging (ISBI)</em> 2022
              <p>
              We introduced a general regularization-based semi-supervised domain generalization method, where the stability and orthogonality of the learned features are introduced as two regularization factors of the learning objective. Both regularization factors can be applied to both labeled and unlabelled data.
              </p>
            </td>

            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/ESAD.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2012.04905">
                <papertitle>ESAD: End-to-end Deep Semi-supervised Anomaly Detection</papertitle>
              </a>
              <br>
              <strong>Chaoqin Huang</strong>, 
              <a href="https://scholar.google.com/citations?hl=en&user=ZuQM_TUAAAAJ">Fei Ye</a>,
              <a href="https://scholar.google.com/citations?user=hCr8Km8AAAAJ&hl=zh-CN">Peisen Zhao</a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
              <a href="https://www.qitian1987.com/">Qi Tian</a>
              <br>
                <em>British Machine Vision Conference (BMVC)</em> 2021
              <p>
              We proposed a new KL-divergence-based objective function and show that two factors: the mutual information, and the entropy, constitute an integral objective function for anomaly detection. Extensive experiments have revealed that the proposed method significantly outperforms several state-of-the-arts on multiple benchmark datasets, including medical diagnosis and several classic anomaly detection benchmarks.
              </p>
            </td>

            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/ICIP.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9506079">
                <papertitle>Deep Unsupervised Image Anomaly Detection: An Information Theoretic Framework</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?hl=en&user=ZuQM_TUAAAAJ">Fei Ye</a>,
              <a href="https://huangjiezheng.com/">Huangjie Zheng</a>,
              <strong>Chaoqin Huang</strong>, 
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>
              <br>
                <em>IEEE International Conference on Image Processing (ICIP)</em> 2021
              <p>
              We proposed an objective function for anomaly detection with information theory, which maximizes the distance between normal and anomalous data in terms of the joint distribution of images and their representation. We managed to find its lower bound which weights the trade-off between mutual information and entropy, which leads to a novel information theoretic framework for unsupervised image anomaly detection.
              </p>
            </td>

            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/guided_filtering_diagram.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drivingstereo-dataset.github.io/">
                <papertitle>DrivingStereo: A Large Dataset to Make Sense of Stereo Matching for Auto-Driving</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=6RaR1cQAAAAJ&hl=zh-CN">Guorun Yang</a>,
              <a href="https://scholar.google.com.hk/citations?user=tXuvWDYAAAAJ&hl=zh-CN">Xiao Song</a>,
              <strong>Chaoqin Huang</strong>, 
              <a href="https://scholar.google.com.au/citations?user=qfewonIAAAAJ&hl=en">Zhidong Deng</a>,
              <a href="https://scholar.google.com/citations?user=mwsxrm4AAAAJ&hl=en">Jianping Shi</a>,
              <a href="https://boleizhou.github.io/">Bolei Zhou</a>
              <br>
                <em>IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR)</em> 2019
              <p>
              We constructed a novel large-scale stereo dataset named DrivingStereo. It contains over 100k images covering a diverse set of driving scenarios. High-quality labels of disparity are produced by a model-guided filtering strategy from multi-frame LiDAR points.
              </p>
            </td>

            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/rrm.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Pan_Recurrent_Residual_Module_CVPR_2018_paper.pdf">
                <papertitle>Recurrent Residual Module for Fast Inference in Videos</papertitle>
              </a>
              <br>
              <a href="http://people.csail.mit.edu/bpan/">Bowen Pan</a>,
              <a href="https://wuwei.io/">Wuwei Lin</a>,
              <a href="https://fang-xiaolin.github.io/">Xiaolin Fang</a>,
              <strong>Chaoqin Huang</strong>, 
              <a href="https://boleizhou.github.io/">Bolei Zhou</a>,
              <a href="https://www.mvig.org/">Cewu Lu</a>
              <br>
                <em>IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR)</em> 2018
              <p>
              We proposed a framework called Recurrent Residual Module (RRM) to accelerate the CNN inference for video recognition tasks. This framework has a novel design of using the similarity of the intermediate feature maps of two consecutive frames, to largely reduce the redundant computation.
              </p>
            </td>

            </tr>
          </tr> 
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading><b></bold>Journals</b></heading> 
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/pami.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>Few-Shot Anomaly Detection via Category-Agnostic Registration Learning</papertitle>
              </a>
              <br>
              <strong>Chaoqin Huang</strong>,
              <a href="https://scholar.google.com/citations?user=Z4bUA3EAAAAJ&hl=en">Haoyan Guan</a>,
              <a href="https://jjjaaafff.github.io/">Aofan Jiang</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
              <a href="https://nms.kcl.ac.uk/michael.spratling/">Michael Spratling</a>,
              <a href="https://sites.google.com/site/sitexinchaowang/">Xinchao Wang</a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>
              <br>
                <em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS) </em> under review
              <p>
              Existing anomaly detection methods require a dedicated model for each category. Such a paradigm is computationally expensive and inefficient. This paper proposes the first FSAD method that requires no model fine-tuning for novel categories: enabling a single model to be applied to all categories. It improves the current state-of-the-art for FSAD by 11.3% and 8.3% on the MVTec and MPDD benchmarks, respectively.
              </p>
            </td>
          </tr> 

          </tr> 
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/layerdecom.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10032792">
                <papertitle>Self-supervised Tumor Segmentation with Sim2Real Adaptation</papertitle>
              </a>
              <br>
              <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <strong>Chaoqin Huang</strong>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              Xin Chen,
              <a href="https://www.qitian1987.com/">Qi Tian</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>
              
              <br>
                <em>IEEE Journal of Biomedical and Health Informatics (JBHI) </em> 2023
              <p>
              We proposed a two-stage Sim2Real training regime for unsupervised tumor segmentation, where we first pre-train a model with simulated tumors and then adopt a self-training strategy for downstream data adaptation. When evaluating on BraTS2018 for brain tumor segmentation and LiTS2017 for liver tumor segmentation, we achieve state-of-the-art segmentation performance under the unsupervised setting.
              </p>
            </td>
          </tr> 

          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/SSM.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9779083">
                <papertitle>Self-Supervised Masking for Unsupervised Anomaly Detection and Localization</papertitle>
              </a>
              <br>
              <strong>Chaoqin Huang</strong>,
              <a href="https://scholar.google.com/citations?user=hQJ5hwMAAAAJ&hl=en&oi=ao">Qinwei Xu</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://mediabrain.sjtu.edu.cn/yuwang/">Yu Wang</a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>
              
              <br>
                <em>IEEE Transactions on Multimedia (TMM) </em> 2022
              <p>
              We proposed a self-supervised learning approach named Self-Supervised Masking (SSM) for unsupervised anomaly detection and localization. SSM not only enhances the training of the inpainting network but also leads to great improvement in the efficiency of mask prediction at inference. We proposed a progressive mask refinement approach that progressively uncovers the normal regions and locates the anomalous regions. The proposed method outperforms several state-of-the-arts, achieving 98.3% AUC on Retinal-OCT (medical diagnosis) and 93.9% AUC on MVTec AD (industrial defect detection), respectively.
              </p>
            </td>
         
            <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()"  bgcolor="#ffffd0">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/ARNet.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1911.10676">
                <papertitle>Attribute Restoration Framework for Anomaly Detection</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?hl=en&user=ZuQM_TUAAAAJ">Fei Ye*</a>,
              <strong>Chaoqin Huang*</strong>,
              <a href="http://www.jinkuncao.com/">Jinkun Cao</a>,
              <a href="https://scholar.google.com/citations?user=Qkx2FKoAAAAJ&hl=zh-CN">Maosen Li</a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://www.mvig.org/">Cewu Lu</a> (* Equal Contribution)
              
              <br>
                <em>IEEE Transactions on Multimedia (TMM) </em> 2022 <font color="red"><strong>(ESI Highly Cited Paper)</strong></font>
              <p>
              We proposed to erase selected attributes from the original data and reformulate the anomaly detection task as a restoration task in the self-supervised learning paradigm, where the normal and the anomalous data are expected to be distinguishable based on restoration errors. By forcing the network to restore the original image, the semantic feature embeddings related to the erased attributes are learned by the network. The proposed method significantly outperforms several state-of-the-arts on multiple benchmark datasets, especially on ImageNet, increasing the AUC of the top-performing baseline by 10.1%.
              </p>
            </td>
            </tr> 
          
        </tbody></table> <br>




        






        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
		Based on a template by <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
